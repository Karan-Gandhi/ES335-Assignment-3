{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next-word Generator for the Linux Kernel C code dataset\n",
    "\n",
    "\n",
    "## Imports and Initial Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 1.11.0+cu113\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import plotly.express as px  # For interactive plotting\n",
    "import plotly.graph_objects as go\n",
    "import re\n",
    "import os\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "# Set up Plotly for better visualization\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = 'notebook_connected'\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Display PyTorch version and set device\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define dataset directory\n",
    "dataset_dir = os.path.join(os.getcwd(), 'datasets')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing C Code and Removing Comments\n",
    "\n",
    "This cell defines two functions to process and tokenize C code: `remove_multiline_comments` and `tokenize_c_code`. \n",
    "\n",
    "- **`remove_multiline_comments`**: \n",
    "    - This function removes all multiline comments (i.e., `/* ... */`) from the input code using regular expressions.\n",
    "    - It uses the `re.DOTALL` flag to match comments across multiple lines.\n",
    "\n",
    "- **`tokenize_c_code`**:\n",
    "    - This function takes a filename containing C code, reads and processes the code, and outputs a list of tokens.\n",
    "    - **Preprocessing Steps**:\n",
    "        - Reads the file and removes any multiline comments with `remove_multiline_comments`.\n",
    "        - Removes single-line comments using a regular expression.\n",
    "    - **Token Specification**:\n",
    "        - The function defines a list `token_specification` that includes patterns for various C components (e.g., keywords, operators, data types).\n",
    "        - Each token pattern is defined as a tuple with the token type and its regex pattern.\n",
    "    - **Token Extraction**:\n",
    "        - All patterns are compiled into a single regex, which is matched iteratively through the code to extract tokens.\n",
    "        - Special tokens (`\\s` for whitespace and `\\n` for newlines) are explicitly added to preserve structure.\n",
    "    - **Output**:\n",
    "        - Returns a list of token values, where newlines are represented as `'\\n'` tokens, and spaces as `'\\s'`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_multiline_comments(code):\n",
    "    \"\"\"\n",
    "    Removes multiline comments from C code.\n",
    "    \"\"\"\n",
    "    multiline_comment_pattern = re.compile(r'/\\*.*?\\*/', re.DOTALL)\n",
    "    code = re.sub(multiline_comment_pattern, '', code)\n",
    "    return code\n",
    "\n",
    "def tokenize_c_code(filename):\n",
    "    \"\"\"\n",
    "    Tokenizes C code into a list of tokens, returning only the token values as strings.\n",
    "    Newlines are represented explicitly as '\\n' tokens.\n",
    "    \"\"\"\n",
    "    # Remove multiline comments\n",
    "    filepath = os.path.join(dataset_dir, filename)\n",
    "    with open(filepath, encoding='utf-8') as file:\n",
    "        code = file.read()\n",
    "        \n",
    "    code = remove_multiline_comments(code)\n",
    "    \n",
    "    # Remove single-line comments\n",
    "    code = re.sub(r'//.*', '', code)\n",
    "    \n",
    "    # Token patterns for various C components\n",
    "    token_specification = [\n",
    "        ('INCLUDE', r'#include'),                             # Capture #include separately\n",
    "        ('HEADER',  r'<[^>]+>'),                              # Capture the header file within angle brackets\n",
    "        ('PREPROCESSOR', r'#\\s*(define|ifdef|ifndef|endif)'), # Other preprocessor directives\n",
    "        ('MACRO', r'#\\s*define\\s+[^\\n]+'),                    # Macro definitions\n",
    "        ('KEYWORD', r'\\b(auto|break|case|char|const|continue|default|do|double|else|enum|extern|'\n",
    "                    r'float|for|goto|if|inline|int|long|register|restrict|return|short|signed|sizeof|'\n",
    "                    r'static|struct|switch|typedef|union|unsigned|void|volatile|while)\\b'),  # Keywords\n",
    "        ('TYPE', r'\\b(int|char|float|double|void|short|long)\\b'),  # Data types\n",
    "        ('IDENTIFIER', r'\\b[A-Za-z_][A-Za-z0-9_]*\\b'),            # Identifiers (variable and function names)\n",
    "        ('NUMBER', r'\\b\\d+(\\.\\d+)?\\b'),                           # Numeric literals (integer or floating-point)\n",
    "        ('STRING_LITERAL', r'\"(?:\\\\.|[^\"\\\\])*\"'),                 # String literals with escaped characters\n",
    "        ('CHAR_LITERAL', r\"'(?:\\\\.|[^'\\\\])'\"),                    # Character literals with escaped characters\n",
    "        ('OPERATOR', r'==|!=|<=|>=|->|&&|\\|\\||\\+\\+|--|'\n",
    "                     r'[+\\-*/%=&|<>!~^]'),                        # Multi-character and single-character operators\n",
    "        ('DELIMITER', r'[;:,.\\[\\]\\(\\)\\{\\}]'),                     # Delimiters\n",
    "        ('NEWLINE', r'\\n'),                                       # Newlines as '\\n' tokens\n",
    "        ('WHITESPACE', r'[ \\t]+'),                                # Skip spaces and tabs\n",
    "        ('MISMATCH', r'.'),                                       # Any other character (error handling)\n",
    "    ]\n",
    "    \n",
    "    # Compile all patterns into a single regex\n",
    "    tok_regex = '|'.join('(?P<%s>%s)' % pair for pair in token_specification)\n",
    "    get_token = re.compile(tok_regex).match\n",
    "\n",
    "    tokens = []\n",
    "    pos = 0\n",
    "    mo = get_token(code)\n",
    "    \n",
    "    while mo is not None:\n",
    "        kind = mo.lastgroup\n",
    "        value = mo.group()\n",
    "\n",
    "        # Add only token values to the tokens list, treating newlines as '\\n'\n",
    "        if kind == 'WHITESPACE':\n",
    "            tokens.append('\\s')  # Explicitly add newline character\n",
    "        elif kind == 'NEWLINE':\n",
    "            tokens.append('\\n')  # Explicitly add newline character\n",
    "        elif kind == 'MISMATCH':\n",
    "            # Handle unexpected characters if needed (log or raise an error)\n",
    "            pass\n",
    "        else:\n",
    "            tokens.append(value)  # Append only the token value\n",
    "        \n",
    "        pos = mo.end()\n",
    "        mo = get_token(code, pos)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Unique Token Mappings\n",
    "\n",
    "The `unique_tokens` function generates unique token mappings from a list of tokens produced by the tokenizer.\n",
    "\n",
    "- **Input**:\n",
    "    - A list of tokens that may contain duplicates.\n",
    "  \n",
    "- **Output**:\n",
    "    - **Vocabulary (`vocab`)**: A sorted list of unique tokens.\n",
    "    - **Mappings**:\n",
    "        - `stoi` (String-to-Index): A dictionary that maps each token in `vocab` to a unique index.\n",
    "        - `itos` (Index-to-String): A dictionary that maps each index back to its corresponding token.\n",
    "  \n",
    "- **Special Tokens**:\n",
    "    - Adds a special `<UNK>` token at the start of `stoi` to represent unknown tokens in future operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_tokens(tokens: list):\n",
    "    \"\"\"\n",
    "    Generates unique token mappings from a list of tokens.\n",
    "    Includes <UNK> token for unknown tokens.\n",
    "    \"\"\"\n",
    "    # Convert to set to get unique tokens, then back to sorted list\n",
    "    vocab = sorted(set(tokens))\n",
    "    \n",
    "    # Initialize stoi with the <UNK> special token\n",
    "    special_tokens = ['<UNK>']\n",
    "    stoi = {token: i + 1 for i, token in enumerate(special_tokens)}\n",
    "    \n",
    "    # Add remaining unique vocabulary tokens, starting from the next available index\n",
    "    next_index = len(stoi) + 1\n",
    "    for token in vocab:\n",
    "        if token not in stoi:\n",
    "            stoi[token] = next_index\n",
    "            next_index += 1\n",
    "    \n",
    "    # Create the itos mapping from updated stoi\n",
    "    itos = {i: s for s, i in stoi.items()}\n",
    "    \n",
    "    return vocab, stoi, itos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Data for Model Training\n",
    "\n",
    "The `prepare_data` function creates input-output pairs from a list of tokens, which are then used to train a language model.\n",
    "\n",
    "- **Input**:\n",
    "    - `tokens`: A list of tokens.\n",
    "    - `block_size`: The context window size, specifying how many tokens each input sequence will contain.\n",
    "    - `stoi`: The string-to-index mapping for tokens.\n",
    "\n",
    "- **Output**:\n",
    "    - Tensors `X` and `Y` for training:\n",
    "        - `X` contains sequences of `block_size` tokens as context.\n",
    "        - `Y` contains the next token following each context as the target.\n",
    "  \n",
    "- **Process**:\n",
    "    - Generates `X` and `Y` by iterating over the `tokens` list and creating context-target pairs.\n",
    "    - Converts each token in the context and target to its corresponding index using `stoi`, with unknown tokens mapped to `<UNK>`.\n",
    "\n",
    "- **Data Conversion**:\n",
    "    - `X` and `Y` are converted into PyTorch tensors for use in model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(tokens: list, block_size: int, stoi):\n",
    "    \"\"\"\n",
    "    Prepares input-output pairs for training from a list of tokens.\n",
    "    Each input consists of `block_size` tokens, and the target is the next token.\n",
    "    Unknown tokens are mapped to the <UNK> index.\n",
    "    \"\"\"\n",
    "    X, Y = [], []\n",
    "    \n",
    "    # Define the <UNK> token\n",
    "    unk_token = '<UNK>'\n",
    "    unk_idx = stoi.get(unk_token, len(stoi) + 1)  # Get or add <UNK> index if not in stoi\n",
    "\n",
    "    # Generate input-output pairs\n",
    "    for i in range(block_size, len(tokens)):\n",
    "        context = tokens[i-block_size:i]\n",
    "        target = tokens[i]\n",
    "        \n",
    "        # Convert context and target to indices, mapping unknown tokens to <UNK>\n",
    "        context_ix = [stoi.get(token, unk_idx) for token in context]\n",
    "        target_ix = stoi.get(target, unk_idx)\n",
    "        \n",
    "        X.append(context_ix)\n",
    "        Y.append(target_ix)\n",
    "    \n",
    "    # Convert lists to tensors\n",
    "    X = torch.tensor(X, dtype=torch.long).to(device)\n",
    "    Y = torch.tensor(Y, dtype=torch.long).to(device)\n",
    "    \n",
    "    return X, Y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Preparation\n",
    "We clean the text, build the vocabulary, and prepare the data for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Clean the text from the dataset\n",
    "text = tokenize_c_code('linux.txt')\n",
    "\n",
    "# Extract unique words and create mappings\n",
    "vocab, stoi, itos = unique_tokens(text)\n",
    "\n",
    "# Add the <UNK> token to itos\n",
    "itos[stoi['<UNK>']] = '<UNK>'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training variants of models:\n",
    "- Embedding size (embedding_dim): 64, 128\n",
    "- Context window size (block size): 5, 10, 15\n",
    "- Activation function: ReLU, Tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter grids\n",
    "embedding_dims = [64, 128]\n",
    "block_sizes = [5, 10, 15]\n",
    "activation_functions = {\n",
    "    'ReLU': nn.ReLU(),\n",
    "    'Tanh': nn.Tanh()\n",
    "}\n",
    "\n",
    "# Other hyperparameters\n",
    "hidden_dim = 512\n",
    "epochs = 500\n",
    "learning_rate = 0.001\n",
    "batch_size = 8196  # Adjust if necessary based on dataset size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the dataloader and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare DataLoader for mini-batch gradient descent\n",
    "def create_data_loader(X, Y, batch_size):\n",
    "    dataset = TensorDataset(X, Y)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# Define the NextWord model with a flexible activation function\n",
    "class NextWord(nn.Module):\n",
    "    \"\"\"\n",
    "    A feedforward neural network with multiple hidden layers for next-word prediction.\n",
    "    Utilizes a configurable activation function to improve gradient flow.\n",
    "    \"\"\"\n",
    "    def __init__(self, block_size, vocab_size, embedding_dim, hidden_dim, activation_fn):\n",
    "        super(NextWord, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lin1 = nn.Linear(embedding_dim * block_size, hidden_dim)\n",
    "        self.lin2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.lin3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.lin4 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.activation = activation_fn\n",
    "        self.lin_out = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeds = self.embedding(x)  # Shape: (batch_size, block_size, embedding_dim)\n",
    "        embeds = embeds.view(x.shape[0], -1)  # Flatten: (batch_size, block_size * embedding_dim)\n",
    "        out = self.activation(self.lin1(embeds))\n",
    "        out = self.activation(self.lin2(out))\n",
    "        out = self.activation(self.lin3(out))\n",
    "        out = self.activation(self.lin4(out))\n",
    "        return self.lin_out(out)  # Shape: (batch_size, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model with Embedding Dim: 64, Block Size: 5, Activation: ReLU\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 904.00 MiB (GPU 0; 4.00 GiB total capacity; 2.84 GiB already allocated; 0 bytes free; 2.86 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7932\\504751472.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     44\u001b[0m                     \u001b[1;31m# Backward pass and optimization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m                     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m                     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m                     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 363\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[1;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[1;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 904.00 MiB (GPU 0; 4.00 GiB total capacity; 2.84 GiB already allocated; 0 bytes free; 2.86 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "\n",
    "# To store loss histories and labels for plotting\n",
    "all_loss_histories = []\n",
    "all_labels = []\n",
    "\n",
    "# Iterate over all combinations of hyperparameters\n",
    "for embedding_dim in embedding_dims:\n",
    "    for block_size in block_sizes:\n",
    "        # Prepare data for the current block_size\n",
    "        X, Y = prepare_data(text, block_size, stoi)\n",
    "        data_loader = create_data_loader(X, Y, batch_size)\n",
    "        \n",
    "        for act_name, act_fn in activation_functions.items():\n",
    "            print(f\"\\nTraining model with Embedding Dim: {embedding_dim}, Block Size: {block_size}, Activation: {act_name}\")\n",
    "            \n",
    "            # Initialize the model\n",
    "            model = NextWord(\n",
    "                block_size=block_size,\n",
    "                vocab_size=len(stoi) + 1,  # +1 for unknown tokens\n",
    "                embedding_dim=embedding_dim,\n",
    "                hidden_dim=hidden_dim,\n",
    "                activation_fn=act_fn\n",
    "            ).to(device)\n",
    "            \n",
    "            # Initialize loss function and optimizer\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "            \n",
    "            # List to store the average loss for each epoch\n",
    "            loss_history = []\n",
    "            \n",
    "            # Training loop with mini-batch gradient descent\n",
    "            for epoch in range(1, epochs + 1):\n",
    "                model.train()  # Set model to training mode\n",
    "                total_loss = 0  # Track total loss for the epoch\n",
    "                \n",
    "                for batch_X, batch_Y in data_loader:\n",
    "                    # Move batches to device\n",
    "                    batch_X, batch_Y = batch_X.to(device), batch_Y.to(device)\n",
    "                    \n",
    "                    # Forward pass\n",
    "                    outputs = model(batch_X)\n",
    "                    loss = loss_fn(outputs, batch_Y)\n",
    "                    \n",
    "                    # Backward pass and optimization\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    # Accumulate loss\n",
    "                    total_loss += loss.item()\n",
    "                \n",
    "                # Average loss per epoch\n",
    "                avg_loss = total_loss / len(data_loader)\n",
    "                loss_history.append(avg_loss)  # Store the average loss\n",
    "            \n",
    "                # Print progress every 100 epochs and at the first epoch\n",
    "                if epoch % 100 == 0 or epoch == 1:\n",
    "                    print(f\"Epoch {epoch}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "            \n",
    "            # Save the model with a unique filename\n",
    "            model_save_path = f'models/c_nextword_model_bs{block_size}_emb{embedding_dim}_act{act_name}.pth'\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            print(f\"Model saved to {model_save_path}\")\n",
    "            \n",
    "            # Store loss history and label for plotting\n",
    "            label = f'Emb={embedding_dim}, BS={block_size}, Act={act_name}'\n",
    "            all_loss_histories.append(loss_history)\n",
    "            all_labels.append(label)\n",
    "\n",
    "# Plotting the training losses for all models\n",
    "fig, axes = plt.subplots(3, 4, figsize=(20, 15))\n",
    "fig.suptitle('Training Loss over Epochs for Various Model Configurations', fontsize=16)\n",
    "\n",
    "for idx, (loss_history, label) in enumerate(zip(all_loss_histories, all_labels)):\n",
    "    row = idx // 4\n",
    "    col = idx % 4\n",
    "    ax = axes[row, col]\n",
    "    ax.plot(range(1, epochs + 1), loss_history, marker='o', markersize=2)\n",
    "    ax.set_title(label)\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Average Loss\")\n",
    "    ax.grid(True)\n",
    "\n",
    "# Hide any unused subplots if total models < subplots\n",
    "total_models = len(all_loss_histories)\n",
    "total_subplots = 3 * 4\n",
    "if total_models < total_subplots:\n",
    "    for idx in range(total_models, total_subplots):\n",
    "        row = idx // 4\n",
    "        col = idx % 4\n",
    "        fig.delaxes(axes[row, col])\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust layout to make room for the main title\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
