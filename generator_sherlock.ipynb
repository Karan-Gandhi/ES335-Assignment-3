{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next-word Generator for the Sherlock Holmes dataset\n",
    "\n",
    "In this assignment, we will build a word-level language model using PyTorch, focusing on improved handling of punctuation and case sensitivity. We will work with the \"Sherlock Holmes\" dataset and visualize word embeddings using Plotly.\n",
    "\n",
    "\n",
    "## Imports and Initial Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 1.11.0+cu113\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import plotly.express as px  # For interactive plotting\n",
    "import plotly.graph_objects as go\n",
    "import re\n",
    "import os\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Set up Plotly for better visualization\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = 'notebook_connected'\n",
    "\n",
    "# Display PyTorch version and set device\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define dataset directory\n",
    "dataset_dir = os.path.join(os.getcwd(), 'datasets')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleaning Function\n",
    "\n",
    "We define a function to clean the text by handling punctuation more effectively and ensuring case insensitivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(filename: str):\n",
    "    \"\"\"\n",
    "    Reads and cleans text from a file.\n",
    "    Handles punctuation by separating them as distinct tokens\n",
    "    and converts all text to lowercase.\n",
    "    \"\"\"\n",
    "    filepath = os.path.join(dataset_dir, filename)\n",
    "    with open(filepath, encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    \n",
    "    # Separate punctuation by adding spaces around them\n",
    "    text = re.sub(r'([.!?])', r' \\1 ', text)\n",
    "    # Remove any unwanted characters except specified punctuation\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s.!?]', ' ', text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove extra spaces\n",
    "    text = \" \".join(text.split())\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unique Words Extraction Function\n",
    "\n",
    "Extracts unique words, including punctuation, and creates mappings between words and their indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_words(text: str):\n",
    "    \"\"\"\n",
    "    Extracts unique words and punctuation from the text.\n",
    "    Creates mappings from string to index and vice versa.\n",
    "    \"\"\"\n",
    "    words = pd.Series(text.split())\n",
    "    # Filter words by length and ensure they are alphabetic or punctuation\n",
    "    words = words[((words.str.len() > 0) & (words.str.len() < 20))]\n",
    "    # Allow punctuation marks as valid tokens\n",
    "    words = words[words.str.match(r'^[a-zA-Z0-9.!?]+$')]\n",
    "    words = words.drop_duplicates(ignore_index=True)\n",
    "    words = words.sort_values(key=lambda x: x.str.len()).reset_index(drop=True)\n",
    "    vocab = words.sort_values().to_list()\n",
    "    \n",
    "    # Create string to index mapping, starting from 1\n",
    "    stoi = {s: i + 1 for i, s in enumerate(vocab)}\n",
    "    stoi['.'] = stoi.get('.', len(stoi) + 1)  # Ensure '.' is included\n",
    "    stoi['!'] = stoi.get('!', len(stoi) + 1)  # Ensure '!' is included\n",
    "    stoi['?'] = stoi.get('?', len(stoi) + 1)  # Ensure '?' is included\n",
    "    \n",
    "    # Create index to string mapping\n",
    "    itos = {i: s for s, i in stoi.items()}\n",
    "    return vocab, stoi, itos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "We prepare the dataset by creating input-output pairs based on a context window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(text: str, block_size: int, stoi):\n",
    "    \"\"\"\n",
    "    Prepares input-output pairs for training.\n",
    "    Each input consists of `block_size` words, and the target is the next word.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    X, Y = [], []\n",
    "    for i in range(block_size, len(words)):\n",
    "        context = words[i-block_size:i]\n",
    "        target = words[i]\n",
    "        # Convert context and target to indices, default to '.' if not found\n",
    "        context_ix = [stoi.get(word, stoi['.']) for word in context]\n",
    "        target_ix = stoi.get(target, stoi['.'])\n",
    "        X.append(context_ix)\n",
    "        Y.append(target_ix)\n",
    "    # Convert lists to tensors\n",
    "    X = torch.tensor(X, dtype=torch.long).to(device)\n",
    "    Y = torch.tensor(Y, dtype=torch.long).to(device)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Preparation\n",
    "We clean the text, build the vocabulary, and prepare the data for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "drop_duplicates() got an unexpected keyword argument 'ignore_index'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_41680\\2067894878.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Extract unique words and create mappings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstoi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munique_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Prepare input-output pairs with a context window of 5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_41680\\2451888576.py\u001b[0m in \u001b[0;36munique_words\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m# Allow punctuation marks as valid tokens\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'^[a-zA-Z0-9.!?]+$'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mvocab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: drop_duplicates() got an unexpected keyword argument 'ignore_index'"
     ]
    }
   ],
   "source": [
    "# Clean the text from the dataset\n",
    "text = clean_text('sherlock.txt')\n",
    "\n",
    "# Extract unique words and create mappings\n",
    "vocab, stoi, itos = unique_words(text)\n",
    "\n",
    "# Prepare input-output pairs with a context window of 5\n",
    "block_size = 5\n",
    "X, Y = prepare_data(text, block_size, stoi)\n",
    "\n",
    "# Display the shapes of the tensors\n",
    "print(f\"Input shape: {X.shape}, dtype: {X.dtype}\")\n",
    "print(f\"Target shape: {Y.shape}, dtype: {Y.dtype}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Initialization and Visualization\n",
    "\n",
    "We initialize the embedding layer and visualize the embeddings using t-SNE with Plotly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Weights Shape: torch.Size([8154, 128])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Perform t-SNE to reduce dimensions to 2D\u001b[39;00m\n\u001b[1;32m     17\u001b[0m tsne \u001b[38;5;241m=\u001b[39m TSNE(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m embeddings_2d \u001b[38;5;241m=\u001b[39m \u001b[43mtsne\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Create a DataFrame for Plotly\u001b[39;00m\n\u001b[1;32m     21\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(stoi\u001b[38;5;241m.\u001b[39mkeys())\n",
      "File \u001b[0;32m/opt/conda_envs/pytorch-ml/lib/python3.10/site-packages/sklearn/utils/_set_output.py:313\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 313\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    315\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    316\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    317\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    318\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    319\u001b[0m         )\n",
      "File \u001b[0;32m/opt/conda_envs/pytorch-ml/lib/python3.10/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda_envs/pytorch-ml/lib/python3.10/site-packages/sklearn/manifold/_t_sne.py:1176\u001b[0m, in \u001b[0;36mTSNE.fit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   1173\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_iter\n\u001b[1;32m   1175\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params_vs_input(X)\n\u001b[0;32m-> 1176\u001b[0m embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_ \u001b[38;5;241m=\u001b[39m embedding\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_\n",
      "File \u001b[0;32m/opt/conda_envs/pytorch-ml/lib/python3.10/site-packages/sklearn/manifold/_t_sne.py:1044\u001b[0m, in \u001b[0;36mTSNE._fit\u001b[0;34m(self, X, skip_num_points)\u001b[0m\n\u001b[1;32m   1038\u001b[0m \u001b[38;5;66;03m# Degrees of freedom of the Student's t-distribution. The suggestion\u001b[39;00m\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;66;03m# degrees_of_freedom = n_components - 1 comes from\u001b[39;00m\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;66;03m# \"Learning a Parametric Embedding by Preserving Local Structure\"\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;66;03m# Laurens van der Maaten, 2009.\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m degrees_of_freedom \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m-> 1044\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tsne\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1045\u001b[0m \u001b[43m    \u001b[49m\u001b[43mP\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1046\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdegrees_of_freedom\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1047\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1048\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_embedded\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_embedded\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1049\u001b[0m \u001b[43m    \u001b[49m\u001b[43mneighbors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneighbors_nn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_num_points\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_num_points\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1051\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda_envs/pytorch-ml/lib/python3.10/site-packages/sklearn/manifold/_t_sne.py:1112\u001b[0m, in \u001b[0;36mTSNE._tsne\u001b[0;34m(self, P, degrees_of_freedom, n_samples, X_embedded, neighbors, skip_num_points)\u001b[0m\n\u001b[1;32m   1110\u001b[0m     opt_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmomentum\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.8\u001b[39m\n\u001b[1;32m   1111\u001b[0m     opt_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_iter_without_progress\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter_without_progress\n\u001b[0;32m-> 1112\u001b[0m     params, kl_divergence, it \u001b[38;5;241m=\u001b[39m \u001b[43m_gradient_descent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mopt_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;66;03m# Save the final number of iterations\u001b[39;00m\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter_ \u001b[38;5;241m=\u001b[39m it\n",
      "File \u001b[0;32m/opt/conda_envs/pytorch-ml/lib/python3.10/site-packages/sklearn/manifold/_t_sne.py:403\u001b[0m, in \u001b[0;36m_gradient_descent\u001b[0;34m(objective, p0, it, max_iter, n_iter_check, n_iter_without_progress, momentum, learning_rate, min_gain, min_grad_norm, verbose, args, kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;66;03m# only compute the error when needed\u001b[39;00m\n\u001b[1;32m    401\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompute_error\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m check_convergence \u001b[38;5;129;01mor\u001b[39;00m i \u001b[38;5;241m==\u001b[39m max_iter \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 403\u001b[0m error, grad \u001b[38;5;241m=\u001b[39m \u001b[43mobjective\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    405\u001b[0m inc \u001b[38;5;241m=\u001b[39m update \u001b[38;5;241m*\u001b[39m grad \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m    406\u001b[0m dec \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39minvert(inc)\n",
      "File \u001b[0;32m/opt/conda_envs/pytorch-ml/lib/python3.10/site-packages/sklearn/manifold/_t_sne.py:284\u001b[0m, in \u001b[0;36m_kl_divergence_bh\u001b[0;34m(params, P, degrees_of_freedom, n_samples, n_components, angle, skip_num_points, verbose, compute_error, num_threads)\u001b[0m\n\u001b[1;32m    281\u001b[0m indptr \u001b[38;5;241m=\u001b[39m P\u001b[38;5;241m.\u001b[39mindptr\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mint64, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    283\u001b[0m grad \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(X_embedded\u001b[38;5;241m.\u001b[39mshape, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m--> 284\u001b[0m error \u001b[38;5;241m=\u001b[39m \u001b[43m_barnes_hut_tsne\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_P\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_embedded\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m    \u001b[49m\u001b[43mneighbors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindptr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mangle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_components\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdof\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdegrees_of_freedom\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    297\u001b[0m c \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2.0\u001b[39m \u001b[38;5;241m*\u001b[39m (degrees_of_freedom \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1.0\u001b[39m) \u001b[38;5;241m/\u001b[39m degrees_of_freedom\n\u001b[1;32m    298\u001b[0m grad \u001b[38;5;241m=\u001b[39m grad\u001b[38;5;241m.\u001b[39mravel()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Define hyperparameters\n",
    "embedding_dim = 128  # Increased embedding size\n",
    "hidden_dim = 512      # Increased hidden layer size\n",
    "batch_size = 4096\n",
    "epochs = 50\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Initialize the embedding layer\n",
    "embedding = nn.Embedding(len(stoi) + 1, embedding_dim).to(device)  # +1 for padding if needed\n",
    "print(f\"Embedding Weights Shape: {embedding.weight.shape}\")\n",
    "\n",
    "# Convert embeddings to NumPy for visualization\n",
    "embeddings = embedding.weight.detach().cpu().numpy()\n",
    "\n",
    "# Perform t-SNE to reduce dimensions to 2D\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "embeddings_2d = tsne.fit_transform(embeddings)\n",
    "\n",
    "# Create a DataFrame for Plotly\n",
    "words = list(stoi.keys())\n",
    "df = pd.DataFrame({\n",
    "    'word': words,\n",
    "    'x': embeddings_2d[:len(words), 0],\n",
    "    'y': embeddings_2d[:len(words), 1]\n",
    "})\n",
    "\n",
    "# Plot using Plotly Express\n",
    "fig = px.scatter(\n",
    "    df, \n",
    "    x='x', \n",
    "    y='y', \n",
    "    text='word',\n",
    "    title='t-SNE Visualization of Word Embeddings',\n",
    "    hover_data=['word'],\n",
    "    width=800,\n",
    "    height=800\n",
    ")\n",
    "\n",
    "# Customize the plot\n",
    "fig.update_traces(textposition='top center', marker=dict(size=5))\n",
    "fig.update_layout(\n",
    "    title=dict(x=0.5),\n",
    "    xaxis_title=\"t-SNE Dimension 1\",\n",
    "    yaxis_title=\"t-SNE Dimension 2\",\n",
    "    template='plotly_white'\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP for next word prediction\n",
    "We define the neural network model with an increased hidden layer size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NextWord(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple feedforward neural network for next-word prediction.\n",
    "    \"\"\"\n",
    "    def __init__(self, block_size, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(NextWord, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)  # Embedding layer\n",
    "        self.lin1 = nn.Linear(embedding_dim * block_size, hidden_dim)  # First hidden layer\n",
    "        self.relu = nn.ReLU()  # Activation function\n",
    "        self.lin2 = nn.Linear(hidden_dim, vocab_size)  # Output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeds = self.embedding(x)  # Shape: [batch_size, block_size, embedding_dim]\n",
    "        embeds = embeds.view(x.shape[0], -1)  # Flatten: [batch_size, block_size * embedding_dim]\n",
    "        out = self.lin1(embeds)  # [batch_size, hidden_dim]\n",
    "        out = self.relu(out)     # [batch_size, hidden_dim]\n",
    "        out = self.lin2(out)     # [batch_size, vocab_size]\n",
    "        return out\n",
    "\n",
    "# Initialize the model with increased hidden dimension\n",
    "model = NextWord(block_size, len(stoi) + 1, embedding_dim, hidden_dim).to(device)\n",
    "print(model)\n",
    "\n",
    "def generate_sequence(model, itos, stoi, context_words, block_size, max_len=20):\n",
    "    \"\"\"\n",
    "    Generates a sequence of words based on the provided context.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    context = [stoi.get(word, stoi['.']) for word in context_words]\n",
    "    if len(context) < block_size:\n",
    "        context = [stoi['.']] * (block_size - len(context)) + context\n",
    "    sequence = context_words.copy()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_len):\n",
    "            x = torch.tensor(context[-block_size:]).unsqueeze(0).to(device)  # Shape: [1, block_size]\n",
    "            y_pred = model(x)  # [1, vocab_size]\n",
    "            probs = F.softmax(y_pred, dim=1)\n",
    "            ix = torch.multinomial(probs, num_samples=1).item()\n",
    "            word = itos.get(ix, '.')\n",
    "            if word == '.':\n",
    "                break\n",
    "            sequence.append(word)\n",
    "            context.append(ix)\n",
    "    \n",
    "    return ' '.join(sequence)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param_name, param in model.named_parameters():\n",
    "    print(f\"{param_name}: {param.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move data to device\n",
    "X = X.to(device)\n",
    "Y = Y.to(device)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()  # Set model to training mode\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X)\n",
    "    loss = loss_fn(outputs, Y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 5 == 0 or epoch == 1:\n",
    "        print(f\"Epoch {epoch}/{epochs}, Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Text Sequences\n",
    "We generate sample text sequences using the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Example context\n",
    "context_words = \"and every precaution has to be taken\"\n",
    "\n",
    "# Generate a sequence of 20 words\n",
    "generated_sequence = generate_sequence(model, itos, stoi, context_words.split(), block_size, max_len=20)\n",
    "\n",
    "print(f\"Context: {context_words}\")\n",
    "print(f\"Generated sequence: {generated_sequence}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-SNE Visualization of Trained Embeddings\n",
    "Finally, we visualize the trained word embeddings using t-SNE with Plotly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Extract trained embeddings\n",
    "trained_embeddings = model.embedding.weight.detach().cpu().numpy()\n",
    "\n",
    "# Perform t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "trained_embeddings_2d = tsne.fit_transform(trained_embeddings)\n",
    "\n",
    "# Create DataFrame for Plotly\n",
    "trained_df = pd.DataFrame({\n",
    "    'word': list(stoi.keys()),\n",
    "    'x': trained_embeddings_2d[:len(stoi), 0],\n",
    "    'y': trained_embeddings_2d[:len(stoi), 1]\n",
    "})\n",
    "\n",
    "# Plot using Plotly Express\n",
    "fig = px.scatter(\n",
    "    trained_df, \n",
    "    x='x', \n",
    "    y='y', \n",
    "    text='word',\n",
    "    title='t-SNE Visualization of Trained Word Embeddings',\n",
    "    hover_data=['word'],\n",
    "    width=800,\n",
    "    height=800\n",
    ")\n",
    "\n",
    "# Customize the plot\n",
    "fig.update_traces(textposition='top center', marker=dict(size=5))\n",
    "fig.update_layout(\n",
    "    title=dict(x=0.5),\n",
    "    xaxis_title=\"t-SNE Dimension 1\",\n",
    "    yaxis_title=\"t-SNE Dimension 2\",\n",
    "    template='plotly_white'\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
