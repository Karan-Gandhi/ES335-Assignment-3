{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next-word Generator for the Taylor Swift Lyrics dataset\n",
    " - Sourced from Kaggle: https://www.kaggle.com/datasets/ishikajohari/taylor-swift-all-lyrics-30-albums\n",
    " - The data was compiled and arranged into a single TaylorSwiftLyrics.txt file.\n",
    " \n",
    "## Imports and Initial Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 1.11.0+cu113\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "# Display PyTorch version and set device\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define dataset directory\n",
    "dataset_dir = os.path.join(os.getcwd(), 'datasets')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleaning Function\n",
    "\n",
    "We define a function to clean the text by handling punctuation more effectively and ensuring case insensitivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "def clean_text(filename: str):\n",
    "    \"\"\"\n",
    "    Reads and cleans text from a file.\n",
    "    Handles punctuation by separating them as distinct tokens,\n",
    "    replaces single or multiple newline boundaries with <PAR_BREAK>, \n",
    "    and converts all text to lowercase.\n",
    "    \"\"\"\n",
    "    filepath = os.path.join(dataset_dir, filename)\n",
    "    with open(filepath, encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    \n",
    "    # Convert to lowercase for case insensitivity\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Separate specified punctuation by adding spaces around them\n",
    "    # Punctuation marks: ., ,, !, ?, -, ', \"\n",
    "    text = re.sub(r'([.,!?\\'\\\"-])', r' \\1 ', text)\n",
    "    \n",
    "    # Remove any unwanted characters except specified punctuation and alphanumerics\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s.,!?\\'\\\"-]', ' ', text)\n",
    "    \n",
    "    # Replace any single or multiple newline characters with <PAR_BREAK>\n",
    "    text = re.sub(r'\\n+', ' <PAR_BREAK> ', text)\n",
    "    # Remove extra spaces\n",
    "    text = \" \".join(text.split())\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unique Words Extraction Function\n",
    "\n",
    "Extracts unique words, including punctuation, and creates mappings between words and their indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def unique_words(text: str):\n",
    "    \"\"\"\n",
    "    Extracts unique words and punctuation from the text.\n",
    "    Creates mappings from string to index and vice versa.\n",
    "    Includes special tokens for paragraph boundaries and <UNK>.\n",
    "    \"\"\"\n",
    "    words = pd.Series(text.split())\n",
    "    \n",
    "    # Define allowed punctuation marks\n",
    "    allowed_punctuations = {'.', ',', '!', '?', '-', '\\'', '\"'}\n",
    "    \n",
    "    # Filter words by length and ensure they are alphanumeric or specified punctuation\n",
    "    words = words[((words.str.len() > 0) & (words.str.len() < 20))]\n",
    "    words = words[words.isin(allowed_punctuations) | words.str.match(r'^[a-zA-Z0-9]+$')]\n",
    "    \n",
    "    # Drop duplicates and sort\n",
    "    words = words.drop_duplicates(ignore_index=True)\n",
    "    vocab = words.sort_values().to_list()\n",
    "    \n",
    "    # Initialize stoi with special tokens\n",
    "    special_tokens = ['<PAR_BREAK>', '<UNK>']\n",
    "    stoi = {token: i + 1 for i, token in enumerate(special_tokens)}\n",
    "    \n",
    "    # Add the remaining vocabulary words, starting from the next available index\n",
    "    next_index = len(stoi) + 1\n",
    "    for word in vocab:\n",
    "        if word not in stoi:\n",
    "            stoi[word] = next_index\n",
    "            next_index += 1\n",
    "    \n",
    "    # Create the itos mapping based on updated stoi\n",
    "    itos = {i: s for s, i in stoi.items()}\n",
    "    \n",
    "    return vocab, stoi, itos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "We prepare the dataset by creating input-output pairs based on a context window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(text: str, block_size: int, stoi):\n",
    "    \"\"\"\n",
    "    Prepares input-output pairs for training.\n",
    "    Each input consists of `block_size` tokens, and the target is the next token.\n",
    "    Unknown words are mapped to the <UNK> token.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    X, Y = [], []\n",
    "    \n",
    "    # Ensure <UNK> and <PAR_BREAK> tokens are in stoi\n",
    "    unk_token = '<UNK>'\n",
    "    par_break_token = '<PAR_BREAK>'\n",
    "    if unk_token not in stoi:\n",
    "        stoi[unk_token] = len(stoi) + 1\n",
    "    if par_break_token not in stoi:\n",
    "        stoi[par_break_token] = len(stoi) + 1\n",
    "\n",
    "    unk_idx = stoi[unk_token]\n",
    "    par_break_idx = stoi[par_break_token]\n",
    "    \n",
    "    for i in range(block_size, len(words)):\n",
    "        context = words[i-block_size:i]\n",
    "        target = words[i]\n",
    "        \n",
    "        # Convert context and target to indices, map unknown words to <UNK>\n",
    "        context_ix = [stoi.get(word, unk_idx) for word in context]\n",
    "        target_ix = stoi.get(target, unk_idx)\n",
    "        \n",
    "        X.append(context_ix)\n",
    "        Y.append(target_ix)\n",
    "    \n",
    "    # Convert lists to tensors\n",
    "    X = torch.tensor(X, dtype=torch.long).to(device)\n",
    "    Y = torch.tensor(Y, dtype=torch.long).to(device)\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Preparation\n",
    "We clean the text, build the vocabulary, and prepare the data for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "drop_duplicates() got an unexpected keyword argument 'ignore_index'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_35352\\486760591.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Extract unique words and create mappings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstoi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munique_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Add the <UNK> and <PAR_BREAK> tokens to `stoi` and `itos` if not present\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_35352\\3614703645.py\u001b[0m in \u001b[0;36munique_words\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;31m# Drop duplicates and sort\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[0mvocab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: drop_duplicates() got an unexpected keyword argument 'ignore_index'"
     ]
    }
   ],
   "source": [
    "# Clean the text from the dataset\n",
    "text = clean_text('lyrics.txt')\n",
    "\n",
    "# Extract unique words and create mappings\n",
    "vocab, stoi, itos = unique_words(text)\n",
    "\n",
    "# Add the <UNK> and <PAR_BREAK> tokens to `stoi` and `itos` if not present\n",
    "if '<UNK>' not in stoi:\n",
    "    unk_idx = len(stoi)\n",
    "    stoi['<UNK>'] = unk_idx\n",
    "    itos[unk_idx] = '<UNK>'\n",
    "\n",
    "if '<PAR_BREAK>' not in stoi:\n",
    "    par_break_idx = len(stoi)\n",
    "    stoi['<PAR_BREAK>'] = par_break_idx\n",
    "    itos[par_break_idx] = '<PAR_BREAK>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training variants of models:\n",
    "- Embedding size (embedding_dim): 64, 128\n",
    "- Context window size (block size): 5, 10, 15\n",
    "- Activation function: ReLU, Tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter grids\n",
    "embedding_dims = [64, 128]\n",
    "block_sizes = [5, 10, 15]\n",
    "activation_functions = {\n",
    "    'ReLU': nn.ReLU(),\n",
    "    'Tanh': nn.Tanh()\n",
    "}\n",
    "\n",
    "# Other hyperparameters\n",
    "hidden_dim = 256\n",
    "epochs = 500\n",
    "learning_rate = 0.001\n",
    "batch_size = 1024  # Adjust if necessary based on dataset size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the dataloader and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare DataLoader for mini-batch gradient descent\n",
    "def create_data_loader(X, Y, batch_size):\n",
    "    dataset = TensorDataset(X, Y)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# Define the NextWord model with a flexible activation function\n",
    "class NextWord(nn.Module):\n",
    "    \"\"\"\n",
    "    A feedforward neural network with multiple hidden layers for next-word prediction.\n",
    "    Utilizes a configurable activation function to improve gradient flow.\n",
    "    \"\"\"\n",
    "    def __init__(self, block_size, vocab_size, embedding_dim, hidden_dim, activation_fn):\n",
    "        super(NextWord, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lin1 = nn.Linear(embedding_dim * block_size, hidden_dim)\n",
    "        self.lin2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.lin3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.lin4 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.activation = activation_fn\n",
    "        self.lin_out = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeds = self.embedding(x)  # Shape: (batch_size, block_size, embedding_dim)\n",
    "        embeds = embeds.view(x.shape[0], -1)  # Flatten: (batch_size, block_size * embedding_dim)\n",
    "        out = self.activation(self.lin1(embeds))\n",
    "        out = self.activation(self.lin2(out))\n",
    "        out = self.activation(self.lin3(out))\n",
    "        out = self.activation(self.lin4(out))\n",
    "        return self.lin_out(out)  # Shape: (batch_size, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stoi' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_35352\\3097294476.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mblock_size\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mblock_sizes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;31m# Prepare data for the current block_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprepare_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblock_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstoi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mdata_loader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_data_loader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'stoi' is not defined"
     ]
    }
   ],
   "source": [
    "# To store loss histories and labels for plotting\n",
    "all_loss_histories = []\n",
    "all_labels = []\n",
    "\n",
    "# Iterate over all combinations of hyperparameters\n",
    "for embedding_dim in embedding_dims:\n",
    "    for block_size in block_sizes:\n",
    "        # Prepare data for the current block_size\n",
    "        X, Y = prepare_data(text, block_size, stoi)\n",
    "        data_loader = create_data_loader(X, Y, batch_size)\n",
    "        \n",
    "        for act_name, act_fn in activation_functions.items():\n",
    "            print(f\"\\nTraining model with Embedding Dim: {embedding_dim}, Block Size: {block_size}, Activation: {act_name}\")\n",
    "            \n",
    "            # Initialize the model\n",
    "            model = NextWord(\n",
    "                block_size=block_size,\n",
    "                vocab_size=len(stoi) + 1,  # +1 for unknown tokens\n",
    "                embedding_dim=embedding_dim,\n",
    "                hidden_dim=hidden_dim,\n",
    "                activation_fn=act_fn\n",
    "            ).to(device)\n",
    "            \n",
    "            # Initialize loss function and optimizer\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "            \n",
    "            # List to store the average loss for each epoch\n",
    "            loss_history = []\n",
    "            \n",
    "            # Training loop with mini-batch gradient descent\n",
    "            for epoch in range(1, epochs + 1):\n",
    "                model.train()  # Set model to training mode\n",
    "                total_loss = 0  # Track total loss for the epoch\n",
    "                \n",
    "                for batch_X, batch_Y in data_loader:\n",
    "                    # Move batches to device\n",
    "                    batch_X, batch_Y = batch_X.to(device), batch_Y.to(device)\n",
    "                    \n",
    "                    # Forward pass\n",
    "                    outputs = model(batch_X)\n",
    "                    loss = loss_fn(outputs, batch_Y)\n",
    "                    \n",
    "                    # Backward pass and optimization\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    # Accumulate loss\n",
    "                    total_loss += loss.item()\n",
    "                \n",
    "                # Average loss per epoch\n",
    "                avg_loss = total_loss / len(data_loader)\n",
    "                loss_history.append(avg_loss)  # Store the average loss\n",
    "            \n",
    "                # Print progress every 100 epochs and at the first epoch\n",
    "                if epoch % 100 == 0 or epoch == 1:\n",
    "                    print(f\"Epoch {epoch}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "            \n",
    "            # Save the model with a unique filename\n",
    "            model_save_path = f'models/lyrics_nextword_model_bs{block_size}_emb{embedding_dim}_act{act_name}.pth'\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            print(f\"Model saved to {model_save_path}\")\n",
    "            \n",
    "            # Store loss history and label for plotting\n",
    "            label = f'Emb={embedding_dim}, BS={block_size}, Act={act_name}'\n",
    "            all_loss_histories.append(loss_history)\n",
    "            all_labels.append(label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting losses for all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 2000x1500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Plotting the training losses for all models\n",
    "fig, axes = plt.subplots(3, 4, figsize=(20, 15))\n",
    "fig.suptitle('Training Loss over Epochs for Various Model Configurations', fontsize=16)\n",
    "\n",
    "for idx, (loss_history, label) in enumerate(zip(all_loss_histories, all_labels)):\n",
    "    row = idx // 4\n",
    "    col = idx % 4\n",
    "    ax = axes[row, col]\n",
    "    ax.plot(range(1, epochs + 1), loss_history, marker='o', markersize=2)\n",
    "    ax.set_title(label)\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Average Loss\")\n",
    "    ax.grid(True)\n",
    "\n",
    "# Hide any unused subplots if total models < subplots\n",
    "total_models = len(all_loss_histories)\n",
    "total_subplots = 3 * 4\n",
    "if total_models < total_subplots:\n",
    "    for idx in range(total_models, total_subplots):\n",
    "        row = idx // 4\n",
    "        col = idx % 4\n",
    "        fig.delaxes(axes[row, col])\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust layout to make room for the main title\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
